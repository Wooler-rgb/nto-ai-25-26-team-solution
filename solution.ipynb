{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Book Recommendation Solution\n\n**Final Score: 0.889 NDCG@20**\n\n### Approach:\n- CatBoost Ranker with PairLogitPairwise loss\n- 150 negatives from top-800 popular books\n- Ensemble of 5 models with rank-based averaging"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRanker, Pool\n",
    "from sklearn.metrics import ndcg_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "targets = pd.read_csv('data/targets.csv')\n",
    "candidates = pd.read_csv('data/candidates.csv')\n",
    "books = pd.read_csv('data/books.csv')\n",
    "users = pd.read_csv('data/users.csv')\n",
    "book_genres = pd.read_csv('data/book_genres.csv')\n",
    "print(f\"Train: {train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats\n",
    "book_stats = train.groupby('book_id').agg({\n",
    "    'has_read': ['sum', 'count', 'mean'],\n",
    "    'rating': 'mean',\n",
    "    'user_id': 'nunique'\n",
    "}).reset_index()\n",
    "book_stats.columns = ['book_id', 'times_read', 'total_interactions', \n",
    "                      'read_rate', 'avg_user_rating', 'unique_users']\n",
    "\n",
    "user_stats = train.groupby('user_id').agg({\n",
    "    'has_read': ['sum', 'mean'],\n",
    "    'rating': 'mean'\n",
    "}).reset_index()\n",
    "user_stats.columns = ['user_id', 'user_books_read', 'user_read_rate', 'user_avg_rating']\n",
    "\n",
    "# Genres\n",
    "user_genres = train[train['has_read'] == 1].merge(book_genres, on='book_id', how='left')\n",
    "user_top_genres = user_genres.groupby(['user_id', 'genre_id']).size().reset_index(name='cnt')\n",
    "user_top_genres = user_top_genres.sort_values(['user_id', 'cnt'], ascending=[True, False])\n",
    "user_top_genres_dict = user_top_genres.groupby('user_id')['genre_id'].apply(list).to_dict()\n",
    "book_genres_dict = book_genres.groupby('book_id')['genre_id'].apply(list).to_dict()\n",
    "\n",
    "def genre_overlap(uid, bid):\n",
    "    ug = set(user_top_genres_dict.get(uid, [])[:10])\n",
    "    bg = set(book_genres_dict.get(bid, []))\n",
    "    if not ug or not bg:\n",
    "        return 0\n",
    "    return len(ug & bg) / len(bg)\n",
    "\n",
    "print(\"Stats ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df = df.merge(book_stats, on='book_id', how='left')\n",
    "    df = df.merge(user_stats, on='user_id', how='left')\n",
    "    df = df.merge(books[['book_id', 'publication_year', 'avg_rating']], on='book_id', how='left')\n",
    "    df = df.merge(users, on='user_id', how='left')\n",
    "    \n",
    "    df['genre_match'] = df.apply(lambda r: genre_overlap(r['user_id'], r['book_id']), axis=1)\n",
    "    df['book_age'] = 2024 - df['publication_year']\n",
    "    df['popularity'] = np.log1p(df['total_interactions'])\n",
    "    df['rating_compatibility'] = 1 - np.abs(df['avg_rating'] - df['user_avg_rating']) / 5.0\n",
    "    \n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "feature_cols = [\n",
    "    'times_read', 'total_interactions', 'read_rate', 'avg_user_rating', 'unique_users',\n",
    "    'user_books_read', 'user_read_rate', 'user_avg_rating',\n",
    "    'publication_year', 'avg_rating', 'book_age',\n",
    "    'age', 'gender',\n",
    "    'genre_match', 'popularity', 'rating_compatibility'\n",
    "]\n",
    "\n",
    "popular_books_800 = book_stats.nlargest(800, 'total_interactions')['book_id'].values\n",
    "user_books_cache = train.groupby('user_id')['book_id'].apply(set).to_dict()\n",
    "\n",
    "print(f\"{len(feature_cols)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Training Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# FINAL CONFIG\nBEST_N_NEG = 150  # Proven to work\nN_MODELS = 5  # More models for rank averaging\nSEEDS = [42, 123, 456, 789, 1000]\n\nprint(f\"Config: N_NEG={BEST_N_NEG}, Ensemble={N_MODELS} models\")\nprint(\"Strategy: Rank-based averaging (more robust)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full train data\n",
    "print(\"Preparing full training data...\")\n",
    "\n",
    "full_train = train.copy()\n",
    "full_train['target'] = full_train['has_read'].apply(lambda x: 2 if x == 1 else 1)\n",
    "\n",
    "np.random.seed(42)\n",
    "full_negatives = []\n",
    "for user_id in full_train['user_id'].unique():\n",
    "    user_books = user_books_cache.get(user_id, set())\n",
    "    neg_candidates = [b for b in popular_books_800 if b not in user_books]\n",
    "    neg_books = np.random.choice(neg_candidates, min(BEST_N_NEG, len(neg_candidates)), replace=False)\n",
    "    for book_id in neg_books:\n",
    "        full_negatives.append({'user_id': user_id, 'book_id': book_id, 'target': 0})\n",
    "\n",
    "full_all = pd.concat([full_train, pd.DataFrame(full_negatives)], ignore_index=True)\n",
    "full_features = create_features(full_all)\n",
    "full_features = full_features.sort_values('user_id').reset_index(drop=True)\n",
    "\n",
    "print(f\"Full train size: {len(full_features)}\")\n",
    "print(f\"Neg:Pos ratio: {len(full_negatives)/len(full_train):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble\n",
    "# Use iterations from previous experiments (100 neg had ~700 iterations)\n",
    "best_iter = 700  # Fixed value, or use results[BEST_N_NEG]['best_iter'] if validation was run\n",
    "print(f\"Training {N_MODELS} models with {best_iter} iterations each...\\n\")\n",
    "\n",
    "full_pool = Pool(full_features[feature_cols], full_features['target'], group_id=full_features['user_id'])\n",
    "\n",
    "final_models = []\n",
    "for i, seed in enumerate(SEEDS[:N_MODELS]):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Model {i+1}/{N_MODELS} (seed={seed})\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    model = CatBoostRanker(\n",
    "        iterations=best_iter,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,\n",
    "        loss_function='PairLogitPairwise',\n",
    "        random_seed=seed,\n",
    "        verbose=100,  # Show progress every 100 iterations\n",
    "        task_type='GPU',\n",
    "        devices='0'\n",
    "    )\n",
    "    model.fit(full_pool)\n",
    "    final_models.append(model)\n",
    "    print(f\"Model {i+1} done!\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Trained {len(final_models)} models!\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test predictions with RANK-based ensemble\nprint(\"Creating test predictions...\")\n\ntest_data = []\nfor _, row in candidates.iterrows():\n    for bid in [int(b) for b in row['book_id_list'].split(',')]:\n        test_data.append({'user_id': row['user_id'], 'book_id': bid})\n\ntest_df = create_features(pd.DataFrame(test_data))\n\n# Get predictions from all models\nall_preds = []\nfor i, model in enumerate(final_models):\n    test_df[f'pred_{i}'] = model.predict(test_df[feature_cols])\n    all_preds.append(f'pred_{i}')\n\n# RANK-based ensemble (more robust than score averaging)\n# For each user, convert scores to ranks, then average ranks\nprint(\"Computing rank-based ensemble...\")\n\ndef rank_ensemble(group):\n    for col in all_preds:\n        # Lower rank = better (rank 1 is best)\n        group[f'rank_{col}'] = group[col].rank(ascending=False)\n    # Average ranks\n    rank_cols = [f'rank_{col}' for col in all_preds]\n    group['avg_rank'] = group[rank_cols].mean(axis=1)\n    return group\n\ntest_df = test_df.groupby('user_id', group_keys=False).apply(rank_ensemble)\n# Lower avg_rank = better, so we sort ascending=True for final ranking\n# But for pred we want higher=better, so negate\ntest_df['pred'] = -test_df['avg_rank']\n\nprint(f\"Test size: {len(test_df)}\")\nprint(\"Done!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create submission\nsubmission = []\nfor uid in targets['user_id']:\n    user_preds = test_df[test_df['user_id'] == uid]\n    user_preds = user_preds.sort_values('pred', ascending=False)\n    user_preds = user_preds.drop_duplicates('book_id', keep='first')\n    top = user_preds.head(20)['book_id'].astype(int).tolist()\n    submission.append({'user_id': uid, 'book_id_list': ','.join(map(str, top))})\n\npd.DataFrame(submission).to_csv('submission.csv', index=False)\nprint(\"\\nsubmission.csv created!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}